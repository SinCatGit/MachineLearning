## 一、kNN算法的工作原理
    
    存在一个样本数据集，也称作训练样本集，并且样本中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。
    输入没有标签的新数据后，将新数据的每个特征与样本集中的数据对应的特征进行比较，然后算法提取样本集中特征最相似的数据（最近邻）的分类标签。
    一般来说，我们只选择样本集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。
    最后，选择k个最相似的数据中出现次数最多的分类，作为新数据的分类。

    通俗理解：k-近邻算法就是根据“新数据的分类取决于它的邻居”进行的，比如邻居中大多数都是程序猿，那么这个人也极有可能是程序猿。
    而算法的目的就是先找出它的邻居，然后分析这几位邻居大多数的分类，极有可能就是它的分类。
    
    
## 二、适用情况
    
    优点：精度高，对异常数据不敏感（类别是由邻居中的大多数决定的，一个异常邻居并不能影响太大），无数据输入假定；

    缺点：计算复杂度高（需要计算新的数据点与样本集中每个数据的“距离”，以判断是否是前k个邻居），空间复杂度高（巨大的矩阵）；

    适用数据范围：数值型（目标变量可以从无限的数值集合中取值）和标称型（目标变量只有在有限目标集中取值）。
 
## 三、学习要点
    
    1. python的numpy库的使用
    2. TODO: matplotlib库的使用